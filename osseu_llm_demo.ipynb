{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2f2b94e2-a93a-465e-b964-f046bf574e27",
      "metadata": {
        "id": "2f2b94e2-a93a-465e-b964-f046bf574e27"
      },
      "source": [
        "# FLAN-T5 XXL Demo\n",
        "### Efficient AI: Empowering Large Language Models with Intel® Extension for PyTorch to Combat Carbon Emissions\n",
        "This demo showcases how Intel® Extension for PyTorch can reduce carbon emissions while maintaining the performance of Large Language Models (LLMs). With just a few lines of code, you will see the improvements that can be achieved in both latency and power consumption. Watch our demo to learn more!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b03aaa8-a70c-4abc-9a16-8e6b139044c1",
      "metadata": {
        "id": "9b03aaa8-a70c-4abc-9a16-8e6b139044c1"
      },
      "source": [
        "##### Optional: Install libraries. You can find the full list of libraries and the version used for this demo in requirements.txt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0ae24aa",
      "metadata": {
        "id": "d0ae24aa"
      },
      "outputs": [],
      "source": [
        "#%pip install gradio\n",
        "#%pip install sentencepiece\n",
        "#%pip install codecarbon\n",
        "#%pip install prometheus_client"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d92665c-9830-449f-ae65-bcf6c4eb4308",
      "metadata": {
        "id": "9d92665c-9830-449f-ae65-bcf6c4eb4308"
      },
      "source": [
        "##### Import libraries and download the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4dae9d43",
      "metadata": {
        "scrolled": true,
        "id": "4dae9d43"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import codecarbon\n",
        "from codecarbon import EmissionsTracker\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "import gradio as gr\n",
        "import time\n",
        "import intel_extension_for_pytorch as ipex\n",
        "\n",
        "# Upload the model\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-xxl\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-xxl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66d92c4e-8523-4d2c-935d-e638b1b9454e",
      "metadata": {
        "id": "66d92c4e-8523-4d2c-935d-e638b1b9454e"
      },
      "source": [
        "##### Create the chat interface:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "415bf598",
      "metadata": {
        "id": "415bf598"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "#Create a version of the model that is optimized by Intel Extension for PyTorch\n",
        "model_o = ipex.optimize(model, dtype=torch.bfloat16)\n",
        "\n",
        "#Gradio demo\n",
        "with gr.Blocks() as demo:\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1, min_width=600):\n",
        "            text1 = gr.Button(value=\"PyTorch* with mkldnn backend turned off\")\n",
        "        with gr.Column(scale=1, min_width=600):\n",
        "            text2 = gr.Button(value=\"PyTorch* + Intel® Extension for PyTorch Optimizer + Auto Mixed Precision\")\n",
        "    with gr.Row():\n",
        "        #Unoptimized performance\n",
        "        with gr.Column(scale=1, min_width=600):\n",
        "            chatbot = gr.Chatbot()\n",
        "            msg = gr.Textbox()\n",
        "            clear = gr.ClearButton([msg, chatbot])\n",
        "\n",
        "            def respond(message, chat_history):\n",
        "                try:\n",
        "                    input_ids = tokenizer(message, return_tensors=\"pt\").input_ids\n",
        "                    with torch.backends.mkldnn.flags(enabled=False):\n",
        "                        tracker = EmissionsTracker()\n",
        "                        tracker.start()\n",
        "                        time1=time.time()\n",
        "                        with torch.no_grad():\n",
        "                            outputs_u = model.generate(input_ids, max_new_tokens=200)\n",
        "                        time2=time.time()\n",
        "                        emissions_no_ipex: float = tracker.stop()\n",
        "                        emissions_no_ipex=emissions_no_ipex*1000000 #convert to mg\n",
        "\n",
        "                    latency_u=time2-time1\n",
        "                    latency_msg=\"Latency: \"+str(latency_u)+\" s\"\n",
        "                    emission_msg=\"Emissions: \"+str(emissions_no_ipex)+\" mgCO2.eq\"\n",
        "                    bot_message = str(tokenizer.decode(outputs_u[0]))+\"\\n\"+latency_msg+\"\\n\"+emission_msg\n",
        "                except Exception as e:\n",
        "                    bot_message = e.__doc__\n",
        "                chat_history.append((message, bot_message))\n",
        "                return \"\", chat_history\n",
        "\n",
        "            msg.submit(respond, [msg, chatbot], [msg, chatbot])\n",
        "\n",
        "        #Optimized performance (model_o)\n",
        "        with gr.Column(scale=1, min_width=600):\n",
        "\n",
        "            chatbot = gr.Chatbot()\n",
        "            msg = gr.Textbox()\n",
        "            clear = gr.ClearButton([msg, chatbot])\n",
        "\n",
        "\n",
        "            def respond(message, chat_history):\n",
        "                try:\n",
        "                    input_ids = tokenizer(message, return_tensors=\"pt\").input_ids\n",
        "                    tracker = EmissionsTracker()\n",
        "                    tracker.start()\n",
        "                    time3=time.time()\n",
        "                    with torch.no_grad(), torch.cpu.amp.autocast():\n",
        "                        outputs_o = model_o.generate(input_ids, max_new_tokens=200)\n",
        "                    time4=time.time()\n",
        "                    emissions_ipex: float = tracker.stop()\n",
        "                    emissions_ipex=emissions_ipex*1000000 #convert to mg\n",
        "\n",
        "                    latency_o=time4-time3\n",
        "                    latency_msg=\"Latency: \"+str(latency_o)+\" s\"\n",
        "                    emission_msg=\"Emissions: \"+str(emissions_ipex)+\" mgCO₂eq\"\n",
        "                    bot_message = str(tokenizer.decode(outputs_o[0]))+\"\\n\"+latency_msg+\"\\n\"+emission_msg\n",
        "\n",
        "                except Exception as e:\n",
        "                    bot_message = e.__doc__\n",
        "\n",
        "                chat_history.append((message, bot_message))\n",
        "                return \"\", chat_history\n",
        "\n",
        "            msg.submit(respond, [msg, chatbot], [msg, chatbot])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be36fba1-49ae-4329-b911-d0cbf14144ac",
      "metadata": {
        "id": "be36fba1-49ae-4329-b911-d0cbf14144ac"
      },
      "source": [
        "##### Launch the demo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1175e726-b0b2-4d9d-a0ee-133cdc95e9e9",
      "metadata": {
        "id": "1175e726-b0b2-4d9d-a0ee-133cdc95e9e9"
      },
      "outputs": [],
      "source": [
        "#share=True if you want a public link to get access to your demo\n",
        "demo.launch(share=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df3c991a-18d8-4e21-9ad1-477ef9f82331",
      "metadata": {
        "id": "df3c991a-18d8-4e21-9ad1-477ef9f82331"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}